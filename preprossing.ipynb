{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np \nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndf = pd.read_csv('cancer.csv')\n\n# Create DataFrame\ndf2 = pd.DataFrame(df)\n\n\"\"\"# Drop the last column\ndf = df.drop(df.columns[-1], axis=1)\"\"\"\n\n# Apply Label Encoding to the diagnosis column\nlabel_encoder = LabelEncoder()\ndf['diagnosis'] = label_encoder.fit_transform(df['diagnosis'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "# checking missing value\nmissing_values=df.isna().sum()\nprint(missing_values)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "id                         0\ndiagnosis                  0\nradius_mean                0\ntexture_mean               0\nperimeter_mean             0\narea_mean                  0\nsmoothness_mean            0\ncompactness_mean           0\nconcavity_mean             0\nconcave points_mean        0\nsymmetry_mean              0\nfractal_dimension_mean     0\nradius_se                  0\ntexture_se                 0\nperimeter_se               0\narea_se                    0\nsmoothness_se              0\ncompactness_se             0\nconcavity_se               0\nconcave points_se          0\nsymmetry_se                0\nfractal_dimension_se       0\nradius_worst               0\ntexture_worst              0\nperimeter_worst            0\narea_worst                 0\nsmoothness_worst           0\ncompactness_worst          0\nconcavity_worst            0\nconcave points_worst       0\nsymmetry_worst             0\nfractal_dimension_worst    0\ndtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "# Check for duplicated rows\nduplicate_rows = df2[df2.duplicated()]\n\nif not duplicate_rows.empty:\n    print(\"Duplicate rows values found! Removing duplicates...\")\n    \nelse:\n    print(\"No duplicate rows values found.\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "No duplicate rows values found.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "# Check for duplicate values in the 'id' column\nduplicate_ids = df.duplicated(subset=['id'])\n\n# Check if there are any duplicate 'id' values\nif duplicate_ids.any():\n    print(\"Duplicate 'id' values found!\")\nelse:\n    print(\"No duplicate 'id' values found.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "No duplicate 'id' values found.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "# outliers removing\n\n# Iterate through each column\nfor col in df.columns:\n    if col != 'diagnosis':  # Exclude 'diagnosis' column\n        if np.issubdtype(df[col].dtype, np.number):  # Check if column is numeric\n            # Define function to remove outliers based on mean for each column\n            def remove_outliers_based_on_mean(column):\n                mean = np.nanmean(column)  # Use np.nanmean() to handle NaN values\n                std_dev = np.nanstd(column)\n                lower_bound = mean - (2.5 * std_dev)\n                upper_bound = mean + (2.5 * std_dev)\n                filtered_column = column[(column >= lower_bound) & (column <= upper_bound)]\n                return filtered_column\n\n            # Apply the function to remove outliers from the column\n            df[col] = remove_outliers_based_on_mean(df[col])\n\n# Drop rows with any NaN values after removing outliers\ndf.dropna(inplace=True)\n\nnum_rows = df.shape[0]\nprint(\"Number of rows in the DataFrame after removing outliers:\", num_rows)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of rows in the DataFrame after removing outliers: 451\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "# checking missing value\nmissing_values=df.isna().sum()\nprint(missing_values)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "id                         0\ndiagnosis                  0\nradius_mean                0\ntexture_mean               0\nperimeter_mean             0\narea_mean                  0\nsmoothness_mean            0\ncompactness_mean           0\nconcavity_mean             0\nconcave points_mean        0\nsymmetry_mean              0\nfractal_dimension_mean     0\nradius_se                  0\ntexture_se                 0\nperimeter_se               0\narea_se                    0\nsmoothness_se              0\ncompactness_se             0\nconcavity_se               0\nconcave points_se          0\nsymmetry_se                0\nfractal_dimension_se       0\nradius_worst               0\ntexture_worst              0\nperimeter_worst            0\narea_worst                 0\nsmoothness_worst           0\ncompactness_worst          0\nconcavity_worst            0\nconcave points_worst       0\nsymmetry_worst             0\nfractal_dimension_worst    0\ndtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": "# Discretization\n\n# Create DataFrame\ndf2 = pd.DataFrame(df)\n\n# Define number of bins\nnum_bins = 3\n\n# Iterate through each column\nfor col in df2.columns:\n    # Exclude 'diagnosis' and 'id' columns\n    if col not in ['diagnosis', 'id']:\n        # Perform discretization using the cut function\n        df2[f'discretized_{col}'] = pd.cut(df2[col], bins=num_bins, labels=False, duplicates='drop')\n\n\nprint(\"Modified DataFrame saved to 'cancer_discretized.csv'\")\n\n# Save the modified DataFrame to a new CSV file\ndf2.to_csv('cancer_preprossing.csv', index=False)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Modified DataFrame saved to 'cancer_discretized.csv'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": "## The reason for using Encodeing\nWe encode the classified column to convert categorical data into a numerical format that can be easily used for analysis or machine learning algorithms and to be ensure that the classification task can be performed accurately and efficiently on the dataset.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## The reason for cleaning data set\nCleaning involves handling missing values, removing duplicates, correcting errors, and dealing with outliers. \nBy cleaning the dataset, we aim to improve data quality, accuracy, and the robustness of any insights or predictions derived from it.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## The reason for using Discretization \nDiscretization simplifies the process of representing data, making it easier to analyze, interpret, and make decisions, and can be useful in some machine learning algorithms.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}